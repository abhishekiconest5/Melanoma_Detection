{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iucIei1Nx0Z7"
      },
      "outputs": [],
      "source": [
        "#Importing the required Deep Learning libraries for Image Processing:\n",
        "import tensorflow as tf;\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator;\n",
        "from tensorflow.keras.models import Sequential;\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense;\n",
        "\n",
        "#Data Upload in Google Colab - As a first step, I have uploaded\n",
        "#the given train images and test images in Google Colab by creating\n",
        "#two sub-directories named \"train_data\" and \"test_data\" under the\n",
        "#default directory named \"sample_data\". Then inside both \"train_data\"\n",
        "#and \"test_data\" I have uploaded the classified and segmented sub-folders\n",
        "#where each sub-folder is mapped to a desease and each sub-folder containing\n",
        "#the shared images.\n",
        "\n",
        "# Define the parameters as mentioned\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "input_shape = (180, 180, 3)  # Assuming 180x180 RGB images\n",
        "\n",
        "# Data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "#Reading the train_data given to train the Deep Learning model\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/sample_data/train_data',  # Directory with training images\n",
        "    target_size=(180, 180),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Define the appropriate CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with optimization and minimising the loss function\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model with appropriate parameters mentioned as starting point\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=32,\n",
        "    epochs=20\n",
        "\n",
        ")\n",
        "\n",
        "# Read the test data shared\n",
        "\n",
        "# Data augmentation and preprocessing\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "#Reading the test dataset shared\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/sample_data/test_data',\n",
        "    target_size=(180, 180),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "# Classify the images from the test dataset\n",
        "predictions = model.predict(test_generator)\n",
        "\n",
        "# Get the class labels\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "#Display the class labels\n",
        "print(class_labels);\n",
        "\n",
        "# Print the model predictions\n",
        "for i in range(len(predictions)):\n",
        "    predicted_class_index = tf.argmax(predictions[i])\n",
        "    predicted_class_label = class_labels[predicted_class_index]\n",
        "    print(f\"Image {i+1}: Predicted class is {predicted_class_label}\")\n",
        "\n"
      ]
    }
  ]
}